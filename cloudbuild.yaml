# cloudbuild.yaml
# CI/CD for data ingestion Cloud Function + Pub/Sub + BigQuery wiring.

steps:
  # STEP 0: Enable required APIs (safe to re-run).
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'gcloud'
    args:
      - 'services'
      - 'enable'
      - 'cloudfunctions.googleapis.com'
      - 'eventarc.googleapis.com'
      - 'run.googleapis.com'
      - 'pubsub.googleapis.com'
      - 'bigquery.googleapis.com'
      - 'secretmanager.googleapis.com'
      - '--project=${PROJECT_ID}'

  # STEP 1: Create Pub/Sub topics (idempotent).
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -euo pipefail
        gcloud pubsub topics create "${_TRIGGER_TOPIC}" --project="${PROJECT_ID}" || true
        gcloud pubsub topics create "${_PUB_SUB_TOPIC}" --project="${PROJECT_ID}" || true

  # STEP 2: Create BigQuery dataset & table (idempotent).
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -euo pipefail
        bq mk --dataset --description "Dataset for quant trading data" "${PROJECT_ID}:${_BIGQUERY_DATASET}" \
          || echo "Dataset ${_BIGQUERY_DATASET} may already exist."
        bq mk \
          --table \
          --description "Raw daily OHLCV bars from Alpaca" \
          --time_partitioning_field timestamp \
          "${PROJECT_ID}:${_BIGQUERY_DATASET}.${_BIGQUERY_TABLE}" \
          ./schemas/daily_bars_schema.json \
          || echo "Table ${_BIGQUERY_TABLE} may already exist."

  # STEP 3: Create Pub/Sub -> BigQuery subscription (idempotent).
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -eu

