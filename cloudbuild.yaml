# cloudbuild.yaml
# CI/CD for data ingestion Cloud Function + Pub/Sub + BigQuery wiring.

steps:
# STEP 0: Enable required APIs (idempotent).
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
    - 'services'
    - 'enable'
    - 'cloudfunctions.googleapis.com'
    - 'eventarc.googleapis.com'
    - 'run.googleapis.com'
    - 'pubsub.googleapis.com'
    - 'bigquery.googleapis.com'
    - 'secretmanager.googleapis.com'
    - '--project=${PROJECT_ID}'

# Step 1: Ensure Pub/Sub topics exist
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # If 'describe' fails, then run 'create'
      gcloud pubsub topics describe run-daily-ingestion || gcloud pubsub topics create run-daily-ingestion
      gcloud pubsub topics describe daily-bars-raw || gcloud pubsub topics create daily-bars-raw

# Step 2: Ensure BigQuery resources exist
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Check/Create Dataset
      bq show quant_data || bq mk -d quant_data
      
      # Check/Create Table
      # IMPORTANT: When creating a table, you must provide a schema. 
      # Ensure you include the path to your schema file (e.g., ./schema.json) at the end of the command.
      bq show quant_data.daily_bars_raw || bq mk -t quant_data.daily_bars_raw [PATH_TO_YOUR_SCHEMA_FILE]

# Step 3: Ensure Pub/Sub subscription exists
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Check/Create Subscription
      gcloud pubsub subscriptions describe daily-bars-raw-bq-sub || \
      gcloud pubsub subscriptions create daily-bars-raw-bq-sub \
        --topic=daily-bars-raw \
        --bigquery-table=${PROJECT_ID}:quant_data.daily_bars_raw
      # Note: We use the built-in ${PROJECT_ID} variable for robustness.

# STEP 4: Deploy the Gen2 Cloud Function with explicit env var flags.
# Example of debugging Step 4
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      echo "--- Debugging Step 4 ---"
      echo "Variable _MY_SETTING is: $_MY_SETTING"
      echo "Variable _LOCATION is: $_LOCATION"
      echo "------------------------"

      # Your original gcloud command goes here...
      # gcloud functions deploy MyFunction --region=$_LOCATION --set-env-vars=SETTING=$_MY_SETTING

- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
    - 'functions'
    - 'deploy'
    - 'ingest-daily-data'
    - '--gen2'
    - '--region=${_GCP_REGION}'
    - '--runtime=python311'
    - '--source=./data_ingestion'
    - '--trigger-topic=${_TRIGGER_TOPIC}'
    - '--entry-point=ingest_daily_data'
    - '--service-account=mjblank2@quant-setup.iam.gserviceaccount.com'
    # Set each environment variable with a separate, explicit flag for robustness.
    - '--set-env-vars=GCP_PROJECT_ID=${PROJECT_ID}'
    - '--set-env-vars=PUB_SUB_TOPIC=${_PUB_SUB_TOPIC}'
    - '--set-env-vars=ALPACA_SECRET_NAME=projects/${_GCP_PROJECT_NUMBER}/secrets/alpaca-api-keys/versions/latest'

# --- Top-level Service Account ---
# This service account will be used to execute all the steps in this build.
serviceAccount: 'mjblank2@quant-setup.iam.gserviceaccount.com'

# --- Substitutions ---
substitutions:
  _GCP_REGION: 'us-central1'
  _TRIGGER_TOPIC: 'run-daily-ingestion'
  _PUB_SUB_TOPIC: 'daily-bars-raw'
  _GCP_PROJECT_NUMBER: '81308911755' # IMPORTANT: Replace with your actual project number if different.
  _BIGQUERY_DATASET: 'quant_data'
  _BIGQUERY_TABLE: 'daily_bars_raw'
  _BIGQUERY_SUB_ID: 'daily-bars-raw-bq-sub'

# --- Build Options ---
options:
  logging: CLOUD_LOGGING_ONLY

