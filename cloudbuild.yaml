# cloudbuild.yaml
# This file defines the Continuous Integration/Continuous Deployment (CI/CD) pipeline
# for our data ingestion Cloud Function and its associated data warehousing components.

steps:
# STEP 1: Enable required APIs.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
    - 'services'
    - 'enable'
    - 'eventarc.googleapis.com'
    - 'bigquery.googleapis.com'
    - 'run.googleapis.com'
    - '--project=${PROJECT_ID}'

# STEP 2: Create the Pub/Sub topics idempotently.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gcloud pubsub topics create ${_TRIGGER_TOPIC} --project=${PROJECT_ID} || true
    gcloud pubsub topics create ${_PUB_SUB_TOPIC} --project=${PROJECT_ID} || true

# STEP 3: Create the BigQuery Dataset and Table for raw daily bars.
# Corrected 'gcloud bigquery' to the proper 'bq' command.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    bq mk --dataset --description "Dataset for quant trading data" ${PROJECT_ID}:${_BIGQUERY_DATASET} || echo "Dataset ${_BIGQUERY_DATASET} already exists."
    bq mk --table --description "Raw daily OHLCV bars from Alpaca" --time_partitioning_field timestamp ${PROJECT_ID}:${_BIGQUERY_DATASET}.${_BIGQUERY_TABLE} ./schemas/daily_bars_schema.json || echo "Table ${_BIGQUERY_TABLE} already exists."

# STEP 4: Create the BigQuery subscription.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gcloud pubsub subscriptions create ${_BIGQUERY_SUB_ID} \
      --topic=${_PUB_SUB_TOPIC} \
      --bigquery-table=${PROJECT_ID}.${_BIGQUERY_DATASET}.${_BIGQUERY_TABLE} \
      --project=${PROJECT_ID} || true

# STEP 5: Deploy the Cloud Function.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
    - 'functions'
    - 'deploy'
    - 'ingest-daily-data'
    - '--gen2'
    - '--region=${_GCP_REGION}'
    - '--runtime=python311'
    - '--source=./data_ingestion'
    - '--trigger-topic=${_TRIGGER_TOPIC}'
    - '--entry-point=ingest-daily-data'
    # This flag explicitly sets the runtime identity for the function.
    - '--service-account=mjblank2@quant-setup.iam.gserviceaccount.com'
    - '--set-env-vars'
    - '^##^GCP_PROJECT_ID=${PROJECT_ID}##PUB_SUB_TOPIC=${_PUB_SUB_TOPIC}##ALPACA_SECRET_NAME=${_ALPACA_SECRET_NAME}'

# --- Top-level Service Account ---
# This service account will be used to execute all the steps in this build.
# The value should be the service account's literal email address.
serviceAccount: 'mjblank2@quant-setup.iam.gserviceaccount.com'

# --- Substitutions ---
substitutions:
  _GCP_REGION: 'us-central1'
  _TRIGGER_TOPIC: 'run-daily-ingestion'
  _PUB_SUB_TOPIC: 'daily-bars-raw'
  _ALPACA_SECRET_NAME: 'projects/${PROJECT_NUMBER}/secrets/alpaca-api-keys/versions/latest'
  _BIGQUERY_DATASET: 'quant_data'
  _BIGQUERY_TABLE: 'daily_bars_raw'
  _BIGQUERY_SUB_ID: 'daily-bars-raw-bq-sub'

# --- Build Options ---
options:
  logging: CLOUD_LOGGING_ONLY


