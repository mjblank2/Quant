# cloudbuild.yaml
# CI/CD for data ingestion Cloud Function + Pub/Sub + BigQuery wiring.

steps:
# STEP 0: Enable required APIs (idempotent).
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
    - 'services'
    - 'enable'
    - 'cloudfunctions.googleapis.com'
    - 'eventarc.googleapis.com'
    - 'run.googleapis.com'
    - 'pubsub.googleapis.com'
    - 'bigquery.googleapis.com'
    - 'secretmanager.googleapis.com'
    - '--project=${PROJECT_ID}'

# Step 1: Ensure Pub/Sub topics exist
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # If 'describe' fails, then run 'create'
      gcloud pubsub topics describe run-daily-ingestion || gcloud pubsub topics create run-daily-ingestion
      gcloud pubsub topics describe daily-bars-raw || gcloud pubsub topics create daily-bars-raw

# Step 2: Ensure BigQuery resources exist
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Check/Create Dataset
      bq show quant_data || bq mk -d quant_data
      
      # Check/Create Table
      # IMPORTANT: Verify the path below. Assuming 'schema.json' is in the root of the repository.
      # If your schema is located elsewhere (e.g., data_ingestion/schema.json), update the path accordingly.
      bq show quant_data.daily_bars_raw || bq mk -t quant_data.daily_bars_raw schema.json

# Step 3: Ensure Pub/Sub subscription exists
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Check/Create Subscription
      gcloud pubsub subscriptions describe ${_BIGQUERY_SUB_ID} || \
      gcloud pubsub subscriptions create ${_BIGQUERY_SUB_ID} \
        --topic=${_PUB_SUB_TOPIC} \
        --bigquery-table=${PROJECT_ID}:${_BIGQUERY_DATASET}.${_BIGQUERY_TABLE}

# STEP 4: DIAGNOSTICS (New Step)
# This step verifies that substitutions are working correctly.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  id: 'VERIFY_SUBSTITUTIONS'
  args:
    - '-c'
    - |
      echo "--- Variable Check ---"
      echo "GCP Region: '${_GCP_REGION}'"
      echo "Trigger Topic: '${_TRIGGER_TOPIC}'"
      echo "Pub/Sub Topic: '${_PUB_SUB_TOPIC}'"
      echo "GCP Project Number: '${_GCP_PROJECT_NUMBER}'"
      echo "------------------------"
      
      # Fail the build immediately if critical variables are empty
      if [[ -z "${_GCP_REGION}" || -z "${_GCP_PROJECT_NUMBER}" || -z "${_TRIGGER_TOPIC}" || -z "${_PUB_SUB_TOPIC}" ]]; then
        echo "ERROR: One or more critical substitution variables are empty."
        echo "Please ensure that the Cloud Build Trigger settings have NO substitution variables defined."
        exit 1
      fi
      echo "All variables populated. Proceeding to deployment."

# STEP 5: Deploy the Gen2 Cloud Function.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
    - 'functions'
    - 'deploy'
    - 'ingest-daily-data'
    - '--gen2'
    - '--region=${_GCP_REGION}'
    - '--runtime=python311'
    - '--source=./data_ingestion'
    - '--trigger-topic=${_TRIGGER_TOPIC}'
    - '--entry-point=ingest_daily_data'
    - '--service-account=mjblank2@quant-setup.iam.gserviceaccount.com'
    # Combine all environment variables into a single, comma-separated flag.
    - '--set-env-vars=GCP_PROJECT_ID=${PROJECT_ID},PUB_SUB_TOPIC=${_PUB_SUB_TOPIC},ALPACA_SECRET_NAME=projects/${_GCP_PROJECT_NUMBER}/secrets/alpaca-api-keys/versions/latest'

# --- Top-level Service Account ---
serviceAccount: 'mjblank2@quant-setup.iam.gserviceaccount.com'

# --- Substitutions ---
# NOTE: This block is ignored if ANY variables are set in the Cloud Build Trigger settings.
substitutions:
  _GCP_REGION: 'us-central1'
  _TRIGGER_TOPIC: 'run-daily-ingestion'
  _PUB_SUB_TOPIC: 'daily-bars-raw'
  _GCP_PROJECT_NUMBER: '81308911755'
  _BIGQUERY_DATASET: 'quant_data'
  _BIGQUERY_TABLE: 'daily_bars_raw'
  _BIGQUERY_SUB_ID: 'daily-bars-raw-bq-sub'

# --- Build Options ---
options:
  logging: CLOUD_LOGGING_ONLY
